일단은 3월중으로 깃허브 크롤링을 해서 간단하게 장고로 구축하는게 목표.
기술스택은 sqlite+django가 될 것 같고, 일단은 노디자인으로 가되 여유가 되면 bootstrap 적용까지 해보자.

beautifulsoup이나 json다루기로 웬만하면 될 것 같다. 깃허브가 api를 제공해주기 때문.
selenium은 필요할 때 다시 찾아보자. 태인이 말로는 도커는 딱히 필요없고 해당 웹드라이버만 설치해서 등록하면 된다고 한다.
sqlite는 굳이 sql 안써도, 장고에서 나오는 makemigrations+migrate로 등록 가능할것같다.

developer.github.com/v3 잘 봐놓자.


========0. 머신러닝을 위한 데이터 처리========

----크롤링, 스크레이핑, 머신러닝----
빅데이터를 분석한다는 것은 수많은 데이터에서 규칙성을 찾는 것.
크롤링 : 프로그램이 웹사이트를 정기적으로 돌며 정보를 추출하는 기술
스크레이핑 : 웹 사이트에 있는 특정 정보를 추출하는 기술
머신러닝 : 4장에서 설명함~

========1. 크롤링과 스크레이핑========

----1-1 데이터 다운로드하기----
웹사이트 데이터 추출 : urllib 라이브러리 사용.
urllib.request : 데이터 접근 담당. 주로 import하게 될 것임.

```
import urllib.request as req

url='뭐시기'
savename='hi.png'

req.urlretrieve(url, savename)
혹은
mem=req.urlopen(url).read()
with open(savename, mode='wb') as f:
  f.write(mem)
```
요런 식으로 간단하게 이미지 다운로드 가능.

그러면 데이터를 추출하려면?
res = req.urlopen(url)
data = res.read()
text = data.decode('utf-8')
print(text)

즉 기본적으로
urlopen(url)로 res를 가져와서,
res.read()로 data를 뽑아내고,
그게 이미지면 파일열어 write(), 텍스트면 decode()해주는 방식.

특정 지역을 알고 싶다면? 쿼리스트링을 이용하면 된다.
이 쿼리스트링을 붙여주려면, urllib.parse 모듈 사용해야 함.

```
import urllib.request as req
import urllib.parse as parse

API='뭐시기'
values={
  'stnId':'108' #지역 : 서울이라는 뜻. api마다 다르겠지?
}

params=parse.urlencode(values) # 알고싶은 values가 urlencode 함수를 거쳐, 쿼리스트링 형태로 params에 저장됨.
url=API+'?'+params
data=req.urlopen(url).read()
text=data.decode('utf-8')
print(text)

```
해주면 서울날씨의 html 정보가 쭈루룩 print됨.
이걸 sys.argv를 이용해, python get.py 108 해주면 가져오는 식으로도 만들 수 있겠지.



----1-2 BeautifulSoup로 스크레이핑하기----
아까는 html과 xml을 그대로 보여주는 것까지 했었다면,
이번엔 거기서 정보를 아예 추출하는 걸 해보자. beautifulsoup를 활용해서!

```
from bs4 import BeautifulSoup

html='''
<html>
<body>
  <h1>안녕</h1>
  <p>나는</p>
  <p>강준우야</p>
  <div id='merong'>메롱</div>
</body>
</html>
'''

soup=BeautifulSoup(html, 'html.parser')

h1=soup.html.body.h1
p1=soup.html.body.p
p2=p1.next_sibling.next_sibling
merong=soup.find(id='merong')

print(h1, p1, p2, merong)

```
식으로 태그 정보를 활용해 거기 있는 데이터를 뽑아낼 수 있음.
next_sibling의 경우 </p>까지 따져주기 때문에, 위 코드에서 두 번 호출한거임.

좀 더 나가보자면


find는 하나만 찾아주는 거고, 다 찾아주고 싶으면 find_all 쓰면 되고.
예를 들어 여러 개의 링크가 있는데, 다 a태그니까
links=soup.find_all('a')로 links에 저장해준 뒤
for a in links:
  href=a.attrs['href'] 로 리다이렉트 url 따내고
  text=a.string으로 내용 따내고

비슷하게 css 선택자(class='뭐시기')로 따낼 수도 있는데, 이 땐 select_one과 select 사용.
<div class="head_info point_up">
  <span class="value">yee</span>
  <span class="txt_krw">x</span>
  <span class="change">x</span>
  <span class="blind">x</span>
</div>
여기서 head_info안의 value를 뽑아내고 싶다면
info = soup.select_one('div.head_info > span.value').string
print(info)
해주면 됨스. id를 뽑을 땐 . 대신 # 사용

html 구조만 확인하면 원하는 요소에 쉽게 접근할 수 있겠지.ㅎㅎ


----1-3 CSS 선택자----
f12 누르고 태그 선택 뒤 오른쪽클릭 > copy selector 꿀팁.
예를 들어 #mw-context-text > ul:nth-child(7) > li > b > a 같이 복잡하게 되어있는 선택자를 한번에 뽑아낼 수 있겠지.
자세한 css선택자 적용법은 35-36p를 참조하자.
정규 표현식을 사용할 수도 있단다.

----1-4 링크에 있는 것을 한꺼번에 내려받기----
상대경로를 절대경로(url)로 바꾸려면 urllib.parse.urljoin() 사용.

from urllib.parse import urljoin

base='https://example.com/html/a.html'
print(urljoin(base, 'b.html'))
print(urljoin(base, 'sub/c.html'))
print(urljoin(base, '../d.html'))
뭐 요런식으로.

urljoin과 재귀를 사용해서, html 페이지 전체를 처리할 수 있음. 순서는 크게
html분석 -> 링크추출 -> 링크처리 -> 파일다운 -> 1번ㄱ if 파일==html
이걸로 파이썬 3.6 docs에 있는 library 문서 전체를 다운받을 수 있는데
download_python_library_docs.py 참조.


========2. 고급 스크레이핑========
----2-1 로그인이 필요한 사이트에서 다운받기----
무상태 HTTP 통신 : 같은 요청에 대한 같은 응답 -> 회원제 사이트를 만들 수 없음.
이걸 보완하기 위한
쿠키 : 클라이언트 컴에 데이터 저장. 용량 적지만 HTTP 헤더에 RW 가능. 보안 취약.
세션 : 쿠키 ID를 키로 사용해 상태통신 구현 가능.

아니근데 실습으로 한빛홈페이지 들어가서, 로그인할때 F12열고 login_proc.php에서 쓰인 데이터 봤는데 비번이 아주 잘보인다.
보안 너무 취약한뎈ㅋㅋㅋㅋㅋㅋ

----2-2 웹 브라우저를 이용한 스크레이핑----
도커 도대체 어떻게 까는거냐.
일단 selenium phantomjs 깔라고 되어있는데 무시하고,
해봤는데 또 드라이버를 따로 깔아야 한다고 하니 나중에 해보자.

----2-3 웹 API로 데이터 추출하기----
스터디카페 오른쪽방 와이파이 너무 느리다.
담부턴 왼쪽방으로 가야지. openweathermap 가입해서 api 테스트 한번 해보자.

----2-4 cron을 이용한 정기적인 크롤링----
cron으로 정기적인 크롤링이 가능하다고 한다. 근데 이거 하려면 nano를 깔아야 하는..
역시나 vm에서 돌아가야 하나 보다.

========3. 데이터 소스의 서식과 가공========
----3-1. 웹의 다양한 데이터 형식----
다양한 형식의 데이터를 분석할 수 있다.
xml 예제 : http://www.kma.go.kr/weather/forecast/mid-term-rss3.jsp?stnId=108
json 예제 : https://api.github.com/repositories
=> repo를 모아둔 api같다. 무작위로 repo의 이름과 소유자를 추출해서 검색해보자.

그 외에도 csv/tsv, yaml를 다룰 수도 있고
Pandas로 데이터 분석할수있고
openpyxl로 파이썬에서 엑셀 파일을 읽고 쓸 수 있다.


----3-2. 데이터베이스----
파이썬으로 다양한 db를 다룰 수 있고,
예시로 sqlite, mysql, tinydb를 다뤘다.